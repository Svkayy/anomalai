# Additional requirements for open-vocabulary CLIP
openai>=1.0.0
transformers>=4.30.0
torch>=2.0.0
torchvision>=0.15.0
pillow>=9.0.0
numpy>=1.21.0
opencv-python>=4.5.0

# Optional: For BLIP-2 integration
accelerate>=0.20.0
bitsandbytes>=0.39.0

# Optional: For other vision-language models
sentence-transformers>=2.2.0
